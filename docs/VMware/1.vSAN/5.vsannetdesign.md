---
layout: default
title: vSAN 网络设计
parent: vSAN
grand_parent: vmware
nav_order: 5
permalink: /docs/vmware/vsan/vsannetworkdesign
---

* content
{:toc}
> 本文是 2016 年初学 vSAN 时的笔记，某些技术细节可能和最新的 vSAN 存在差异，请结合最新的产品文档/特性查看



------



参考文章：vmware-virtual-san-network-design-guide-white-paper

此部分在实际部署时，请确保交换机管理员理解文中提到的配置要点。

一句话总结：**保证**一个VSAN集群在一个二层网络内，之间使用全线速交换机(交换机堆叠)连接。



# 简介

VSAN 集群中的所有主机必须配置VSAN网络才能使用VSAN存储，无论此主机是否为VSAN提供存储资源。

VSAN 网络属于vSphere网络的一部分，无法将其从vSphere的网络服务中独立出来。VSAN 支持虚拟标准交换机(VSS)和虚拟分布式交换机(VDS)。通常VSAN的网络可能和其他vSphere网络流量公用虚拟或者物理网络资源，因此需要谨慎地进行设计。



# **物理承载网络**

## **网络结构**

传统的网络使用**核心-汇聚-接入**三层网络架构。此种结构有较高的稳定性和冗余性，但是受制于传统二层网络的缺点，这种网络结构需要使用STP防止环路，因此会有一半的物理链路可能不会进行数据传输，造成链路浪费。

此种网络结构适合于南北流量较大的场合，而随着虚拟化的发展，如果要做服务器虚拟化，则所有承载相同业务虚机的主机必须属于一个Pod内，也就是在一个二层广播域内。



越来越多的数据中心开始采用鱼骨架网络结构，置顶交换机作为整个机柜的网关，这样的结构可以避免STP引起的链路浪费，且能减小广播域，减少二层网络过大带来的风险。同时这种结构非常利于横向扩展。

这种结构面对服务器虚拟化带来的冲突就是，大部分虚拟化需要二层网络结构，即一个集群内的生产网络需要处于同一个广播域内，解决此问题有两种办法：

1、在二层之下运行选路协议，构建无环的大二层结构，这类技术有思科的 FabricPath、标准的TRILL（华为等使用）、Juniper Qfabric。

2、使用VxLAN技术(例如VMware的NSX)，这样对底层承载网络的需求就只是 IP 可达。



VSAN 同时支持以上两种网络结构。**但是推荐使用大二层结构，所有VSAN集群的主机在同一个二层组播内。**



## **东西向流量设计考量：**

VMware VSAN 需要主机之间有低延迟、高带宽的网络。当前很多线速置顶交换机都可以满足这一需求。



在鱼骨架结构中，受限于全互联结构和端口密度，一个Leaf交换机的多个下行口可能共享少数的几个上联口。造成pod和pod之间总带宽受限。如下图所示，4个万兆口对应16个万兆下行，也就是平均每个下行端口只能得到2.5Gbps的带宽。

举个例子，假如三个机架组成三个故障域，当前每个故障域需要rebuild 6TB的数据，机柜和机柜之间30Gbps的带宽，假设磁盘性能没有瓶颈。完成此操作大概需要26分钟。但是如果同样的数据，机柜间的带宽是10G，则需要80分钟

所以使用此结构的时候，一定要考虑pod和pod之间的带宽，尽可能提高以避免rebuild，高并发读写等动作发生时网络拥塞。



当做跨数据中心的VSAN（例如VSAN延展集群）时，需要注意的是数据中心之间的带宽。

做VSAN延展集群最大的流量发生在重构时，此时主机间会全速读写，此时数据中心之间的带宽尤其重要。

如果要同步12T的数据，使用1Gbps的网络需要24个小时才能完成(关于延展集群的设计[请看这里](http://bandari.fans.blog.163.com/blog/static/16941890620166444338518/))。



## **Cisco FEX/Nexus 2000 设计考量：**

思科Nexus 2000这类FEX设备需要另行考虑，Nexus2000交换机并不具备本地流量转发功能，所有流量必须通过上行FEX口将其传给Nexus5000或者Nexus7000交换机进行交换。这样同一个Nexus2k交换机上两台主机之间的通信会有延迟。更大的问题是，通常FEX接口带宽有限，如果发生VSAN rebuild这样的大数据量读写操作，FEX口很容易成为带宽瓶颈。

**VMware建议使用将VSAN集群内的所有主机连接到一台交换机（或一组堆叠交换机），如果环境限制必须使用多个互联的交换机，一定要注意交换机互联口的带宽！**



## **Flow Control**

停顿帧(Pause Frame) 通常被用于以太网内流量控制。有时候一台ESXi(或交换机)发送数据的速度大于接收端ESXI 的接收速度， 这时接收端会发生拥塞，接收端此时会给发送端发送停顿帧，让其短暂停止发送数据。



VSAN 使用人造延迟来避免拥塞发生，因为VSAN自身有流控功能，所以推荐关闭用于传输VSAN数据的VMkernel的 Flow Control 功能。默认流控会在所有物理uplink开启。更多关于Flow Control的信息请看[KB 1013413](https://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1013413)。

 **VMware 建议关闭 VSAN vmkernel 的 Flow Control。**



## **安全考量**

VSAN 通过 IP 网络传输存储数据，明文传输，为了保证数据安全，可以考虑将 VSAN 网络隔离部署；或者在操作系统上使用加密软件。



## **主机网络适配器**

每个VSAN节点都应该考虑到以下内容：

至少需要一个用于VSAN流量的网卡，建议使用多个提供链路冗余。这些物理网卡可以和其他流量(vMotion，业务流量)混跑。

当多种流量公用一张物理网卡时，建议对VSAN流量使用VLAN隔离。可以使用Network IO control（NIOC）来做QoS。

All-Flash 下推荐使用10G及以上速度的网卡，Hybrid模式下可以使用1G的网卡，但是一定要专用于VSAN流量。

VSAN 可以使用 25/40/100G 的网卡，前提是 vSphere 支持这些网卡。



# **虚拟网络**

## **VMkernel 网络 及 vSwitch**

在 VSAN 中，引入了名为 Virtual SAN traffic 的 VMkernel 类型，VSAN 集群中所有主机必须设置此VMkernel。VSAN VMkernel 可以放在VSS以及VDS中，配置时确保这些VMkernel所属的port-group配置相同。

vSphere支持[多网卡(multi-NIC)vMotion](https://kb.vmware.com/selfservice/search.do?cmd=displayKC&docType=kc&docTypeID=DT_KB_1_1&externalId=2078252)，以此实现多VMkernel负载均衡；VSAN 则不行，只能有一个 VMkernel 。



VSAN包含 VDS 的使用许可，很多服务例如LLDP、LACP、CDP、NIOC 都依赖于VDS。且VDS有很多高级管理功能，所以**VSAN环境下推荐使用VDS**。



## **NIC Teaming**

VSAN 可以使用 Teaming 和故障切换策略来确定流量如何转发，在发现故障时流量如何重定向。

在VSAN环境下Teaming**只是用来提供冗余而不是负载均衡**。但是如何有其他流量和VSAN公用一个绑定组，则这些流量可以正常使用网卡绑定中的策略。



## **组播**

详细的VSAN组播介绍可以看[此文档](https://www.vmware.com/files/pdf/products/vsan/vmware-vsan-layer2-and-layer3-network-topologies.pdf) 。

VSAN 使用组播在集群中各个节点间传送metadata，以此减少系统开销，减少带宽占用。物理网络是二层以及三层组播都受VSAN支持，但是为了减少网络复杂性VSAN建议使用二层网络。

在二层网络中，**交换机需要开启 IGMP snooping 防止组播流量发送到不必要的接口**；**三层网络中需要使用 PIM-SM 或 PIM-DM保证组播跨三层通信。**

VSAN 默认使用 IGMP v3，如果使用V3通信故障，则会回退到v2，建议在一个集群内，使用相同的IGMP版本。

默认VSAN集群创建后会生成一个默认的组播地址，当同一二层网络中有多个VSAN集群时，需要手动修改默认的组播地址，避免流量冲突。改变此组播地址的时候，也要注意不要和其他服务的组播地址冲突，例如VXLAN也使用组播。

如果一个集群的多个主机并不在一个网段，而是跨三层连接，建议修改默认的Multicast group地址和Multicast Agent地址。

VSAN 建议使用 239.0.0.0/8这个段内的组播地址。	

具体调整方法可以参照此KB： https://kb.vmware.com/kb/2075451

**多个VSAN集群存在时，最简单的隔离办法是使用VLAN隔离**。



## **NIOC （network IO control）**

当VSAN同其他流量混跑的时候，可以通过VDS的NIOC实行QoS，默认NIOC的配置如下，未对VSAN流量进行任何限制。

建议如果要配置NIOC，遵照下列原则：

- 带宽分配上，使用份额(Shares)，**不要配置限制(Limit)**，前者更能有效利用网络资源。

- 针对FT流量要设置一个较高的值，因为FT对延迟相当敏感。

- NIOC 结合端口组的流量调整实现更高级的带宽控制。



## **Jumbo Frame**

VSAN支持Jumbo Frame，但是并不依赖。使用Jumbo Frame可以减少CPU资源使用，但是效果甚微，因为vSphere有TSO和LRO （更多信息请看KB2055140）提供类似的功能。如果当前数据中心网络已经开启了Jumbo Frame，那么在部署VSAN时可以考虑使用Jumbo Frame。如果网络未配置Jumbo Frame，可以不去使用Jumbo Frame。



## **LLDP 与 CDP**

VSS支持CDP(思科设备的邻居发现协议)，VDS同时支持LLDP和CDP，开启LLDP或者CDP后，在VSS或者VDS可以看到交换设备的信息，**建议开启此功能**，且配置为both(同时发送及接收发现报文)模式。



## **三层VSAN网络**

如果要使用VSAN延展集群(Stretched Cluster)，大部分时候主机之间的网络都是跨越三层的，此时不仅要注意物理网络的组播问题，也要考虑到VSAN vmkernel之间的通信问题。VSAN和管理网络公用一个TCP/IP 堆栈，共享一个默认网关，如果在特殊情况下，VSAN主机之间管理网络通信受限，则需要修改ESXi的路由表，让跨网段的VSAN网络可以相互通信。

关于延展集群的设计请看下一篇。
